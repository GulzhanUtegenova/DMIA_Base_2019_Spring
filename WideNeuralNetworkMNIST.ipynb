{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WideNeuralNetworkMNIST.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "tBkZWSFP5ehX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Нейросеть с одним скрытым слоем на MNIST"
      ]
    },
    {
      "metadata": {
        "id": "NIIN55xK5ehY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Попробуем применить модель с одним скрытым слоем на датасете MNIST. Необходимо будет реализовать функцию, обучающую модель и понять, в какое качество мы \"упираемся\"."
      ]
    },
    {
      "metadata": {
        "id": "IoZUw2sU5eha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Загрузим данные"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ebU7H40d5ehb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UhMKXSZ95ehf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Для воспроизводимости расчетов воспользуемся стандартным разбиением на обучающую и тестовую выборки"
      ]
    },
    {
      "metadata": {
        "id": "a3kHa26G5ehg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, test = mnist.load_data()\n",
        "\n",
        "x_train = train[0]\n",
        "y_train = train[1]\n",
        "\n",
        "x_test = test[0]\n",
        "y_test = test[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IEOwyJa95ehk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Перед тем, как начать работу, посмотрите на данные глазами"
      ]
    },
    {
      "metadata": {
        "id": "VCMlZx-z5ehl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "29714449-3fb2-4cca-977e-1ea2407fdc8f"
      },
      "cell_type": "code",
      "source": [
        "images_and_labels = list(zip(x_train,  y_train))\n",
        "for index, (image, label) in enumerate(images_and_labels[:12]):\n",
        "    plt.subplot(5, 4, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('label: %i' % label )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAACuCAYAAACPxT46AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "hdXZvTET5ehp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Преобразуем данные: сделаем так, чтобы мы работали с матрицей, у которой значения от 0 до 1"
      ]
    },
    {
      "metadata": {
        "id": "TKPzFNSt5ehq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "415a4ea4-09e1-4b3b-9f72-2eae865ad94f"
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(60000, 28*28)\n",
        "x_test = x_test.reshape(10000, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwriPPSf5ehw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А таргет сделаем категориальной переменной (то есть значение таргета по индексу k будет говорить, является ли эта цифра k)"
      ]
    },
    {
      "metadata": {
        "id": "3BuyW5ro5ehx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9cqgAD15eh1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Построим модель для обучения"
      ]
    },
    {
      "metadata": {
        "id": "pDRQd35_5eh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Зафиксируем гиперпараметры сети"
      ]
    },
    {
      "metadata": {
        "id": "wCr6-VoS5eh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "G3ow8B7-5eh_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracies_of_wide_model(units, data, batch_size=128, epochs=10, n_iterations=5):\n",
        "    \"\"\"\n",
        "    Функция создает модель с одним скрытым слоем с количеством вершин units,\n",
        "    обучается на данных data с гиперпараметрами batch_size и epochs количество\n",
        "    раз, равное n_iterations и возвращает массив метрик качества на каждой итерации.\n",
        "    \n",
        "    Для функции активации используйте relu, на последнем слое используйте softmax.\n",
        "    \n",
        "    :param units: количество вершин (регулируем)\n",
        "    :param batch_size: размер батча (берем по дефолту)\n",
        "    :param epochs: количество эпох (берем по дефолту)\n",
        "    :param n_iterations: количество итераций (берем по дефолту)\n",
        "    :param data: кортеж, (x_train, y_train, x_test, y_test)\n",
        "    :return: массив, качество на тестовой выборке по каждой итерации\n",
        "    \"\"\"\n",
        "    \n",
        "    x_train, y_train, x_test, y_test = data\n",
        "    accuracies = []\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        model = Sequential()\n",
        "        # добавление необходимых слоев\n",
        "        model.add(Dense(1024, activation=\"relu\", input_shape=(784,)))\n",
        "        model.add(Dense(10, activation=\"softmax\"))\n",
        "        # компилируем модель (не меняйте параметры)\n",
        "        model.compile(optimizer=\"adam\",\n",
        "                loss='categorical_crossentropy',\n",
        "                 metrics=[\"accuracy\"])\n",
        "        \n",
        "        # обучение (указать правильные параметры)\n",
        "        history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "        #получение конечного качества на тестовой выборке\n",
        "        accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "        \n",
        "        \n",
        "        accuracies.append(accuracy)\n",
        "    \n",
        "    return accuracies[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UgQrF_Pe5eiG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Переберем разное количество вершин в нашей сети. Чтобы сильно не напрягать компьютер, не берите больше 3000 вершин.\n",
        "\n",
        "Задача со звездочкой: постройте график качества и посмотрите, изменяется ли оно"
      ]
    },
    {
      "metadata": {
        "id": "V5e8JeyL5eiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1802
        },
        "outputId": "62aada16-5860-4eec-eae0-c2f22860402c"
      },
      "cell_type": "code",
      "source": [
        "# делаем массив вершин (рекомендуем брать до 3000)\n",
        "units_list = [3000]\n",
        "              \n",
        "data = (x_train, y_train, x_test, y_test)\n",
        "results = [get_accuracies_of_wide_model(units, data) for units in units_list]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.2376 - acc: 0.9318 - val_loss: 0.1255 - val_acc: 0.9622\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0899 - acc: 0.9730 - val_loss: 0.0937 - val_acc: 0.9717\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0574 - acc: 0.9825 - val_loss: 0.0715 - val_acc: 0.9783\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0391 - acc: 0.9880 - val_loss: 0.0668 - val_acc: 0.9801\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0269 - acc: 0.9920 - val_loss: 0.0627 - val_acc: 0.9818\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0190 - acc: 0.9948 - val_loss: 0.0609 - val_acc: 0.9809\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0137 - acc: 0.9960 - val_loss: 0.0641 - val_acc: 0.9803\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0104 - acc: 0.9973 - val_loss: 0.0673 - val_acc: 0.9803\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0619 - val_acc: 0.9828\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0704 - val_acc: 0.9822\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 12s 202us/step - loss: 0.2372 - acc: 0.9318 - val_loss: 0.1152 - val_acc: 0.9661\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0911 - acc: 0.9732 - val_loss: 0.0788 - val_acc: 0.9751\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0578 - acc: 0.9826 - val_loss: 0.0693 - val_acc: 0.9781\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0400 - acc: 0.9877 - val_loss: 0.0656 - val_acc: 0.9797\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0263 - acc: 0.9923 - val_loss: 0.0683 - val_acc: 0.9795\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0197 - acc: 0.9943 - val_loss: 0.0647 - val_acc: 0.9798\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0675 - val_acc: 0.9787\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.0713 - val_acc: 0.9803\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0085 - acc: 0.9978 - val_loss: 0.0688 - val_acc: 0.9815\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0068 - acc: 0.9983 - val_loss: 0.0630 - val_acc: 0.9824\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.2395 - acc: 0.9317 - val_loss: 0.1109 - val_acc: 0.9672\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0910 - acc: 0.9733 - val_loss: 0.0844 - val_acc: 0.9741\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0574 - acc: 0.9830 - val_loss: 0.0735 - val_acc: 0.9763\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0375 - acc: 0.9889 - val_loss: 0.0592 - val_acc: 0.9819\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0272 - acc: 0.9922 - val_loss: 0.0606 - val_acc: 0.9801\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0604 - val_acc: 0.9813\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0157 - acc: 0.9955 - val_loss: 0.0597 - val_acc: 0.9811\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.0619 - val_acc: 0.9819\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0097 - acc: 0.9975 - val_loss: 0.0720 - val_acc: 0.9805\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0067 - acc: 0.9983 - val_loss: 0.0718 - val_acc: 0.9801\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.2383 - acc: 0.9312 - val_loss: 0.1207 - val_acc: 0.9634\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0915 - acc: 0.9723 - val_loss: 0.0837 - val_acc: 0.9752\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0575 - acc: 0.9826 - val_loss: 0.0692 - val_acc: 0.9782\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0394 - acc: 0.9881 - val_loss: 0.0603 - val_acc: 0.9813\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0281 - acc: 0.9917 - val_loss: 0.0619 - val_acc: 0.9806\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.0203 - acc: 0.9940 - val_loss: 0.0609 - val_acc: 0.9814\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0158 - acc: 0.9955 - val_loss: 0.0619 - val_acc: 0.9804\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.0647 - val_acc: 0.9822\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0718 - val_acc: 0.9812\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0083 - acc: 0.9977 - val_loss: 0.0755 - val_acc: 0.9785\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 12s 202us/step - loss: 0.2368 - acc: 0.9318 - val_loss: 0.1135 - val_acc: 0.9657\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0918 - acc: 0.9734 - val_loss: 0.0963 - val_acc: 0.9712\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0573 - acc: 0.9831 - val_loss: 0.0673 - val_acc: 0.9774\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0394 - acc: 0.9884 - val_loss: 0.0636 - val_acc: 0.9809\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0275 - acc: 0.9920 - val_loss: 0.0688 - val_acc: 0.9784\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0197 - acc: 0.9943 - val_loss: 0.0589 - val_acc: 0.9826\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0149 - acc: 0.9960 - val_loss: 0.0617 - val_acc: 0.9802\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0656 - val_acc: 0.9812\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.0082 - acc: 0.9980 - val_loss: 0.0726 - val_acc: 0.9795\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0663 - val_acc: 0.9818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9GRCKEhV5eiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ответы для формы"
      ]
    },
    {
      "metadata": {
        "id": "YO70uVyP5eiO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ответом для формы должно служить максимальное качество на тестовой выборке. Поскольку в keras результаты разнятся от запуска к запуску, правильный ответ будет засчитан как интервал"
      ]
    },
    {
      "metadata": {
        "id": "WMQiLZ0J5eiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6907dc8a-a0d6-4eef-b27d-d87a6c4a37ca"
      },
      "cell_type": "code",
      "source": [
        "max_results = max([max(result) for result in results])\n",
        "print('{:.4f}'.format(max_results))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9822\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}